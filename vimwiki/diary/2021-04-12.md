# scrapy.spiders.Rule

```
class Rule:

    def __init__(
        self,
        link_extractor=None,
        callback=None,
        cb_kwargs=None,
        follow=None,
        process_links=None,
        process_request=None,
        errback=None,
    ):
        self.link_extractor = link_extractor or _default_link_extractor
        self.callback = callback
        self.errback = errback
        self.cb_kwargs = cb_kwargs or {}
        self.process_links = process_links or _identity
        self.process_request = process_request or _identity_process_request
        self.follow = follow if follow is not None else not callback

    def _compile(self, spider):
        self.callback = _get_method(self.callback, spider)
        self.errback = _get_method(self.errback, spider)
        self.process_links = _get_method(self.process_links, spider)
        self.process_request = _get_method(self.process_request, spider)

```

实例化一个Rule:
```
Rule(link_extractor= ,callback= , follow= )
```


Rule的作用:

- 按规则(re,xpath,css等),解析出url,并构造Request对象发起请求,获取Response响应对象后将其交给指定的回调函数(如果有的话),并决定是否继续从其响应中解析url(follow)

理解follow:
rules就是作用于CrawlSpider默认的解析函数parse的，如果follow为True就像当于除了将response传入用户定义的回调函数callback之外还将其传入本身默认的解析函数parse


link_extractor:指定链接提取器：LinkExtractor()
不管是否指定了`callback`回调函数，

rules = list[Rule] or tuple(Rule)

rules里的所有规则会对所有返回的response生效

> If callback is None follow defaults to True, otherwise it defaults to False.
follow是决定LinkExtractor 解析出来的页面对应的响应是否继续交给rules解析的参数，`True`为继续交给rules解析

CrawlSpider 类是继承Spider类的，那么它也是通过 start_requests()发起请求,获取的Response对象会交给`_parse_response()`方法处理，从源码中可以看到，未指定callback,会直接进入follow流程,未指定callback,follow默认为True,那么通过rules解析出来的url发起的请求所对应的响应会再次交给rules解析url,直到再也解析不到url为止


那么follow的作用就是决定通过rules解析的url对应的响应是否会再次交给rules解析:
笔趣阁的案例：
一个分页Link_Extraction解析器设置follow为True,才能在分页中不断提取其他分页的url,
详情页的Link_Extraction解析器设置follow为True，详情页对应的页面才会被继续交给rules解析，
解析章节地址的Link_Extraction解析器才能从详情页的响应对象中解析每一章的url

CrawlSpider类源码
```
"""
This modules implements the CrawlSpider which is the recommended spider to use
for scraping typical web sites that requires crawling pages.

See documentation in docs/topics/spiders.rst
"""

import copy
from typing import Sequence

from scrapy.http import Request, HtmlResponse
from scrapy.linkextractors import LinkExtractor
from scrapy.spiders import Spider
from scrapy.utils.spider import iterate_spider_output

def _identity(x):
    return x


def _identity_process_request(request, response):
    return request


def _get_method(method, spider):
    if callable(method):
        return method
    elif isinstance(method, str):
        return getattr(spider, method, None)


_default_link_extractor = LinkExtractor()


class Rule:

    def __init__(
        self,
        link_extractor=None,
        callback=None,
        cb_kwargs=None,
        follow=None,
        process_links=None,
        process_request=None,
        errback=None,
    ):
        self.link_extractor = link_extractor or _default_link_extractor
        self.callback = callback
        self.errback = errback
        self.cb_kwargs = cb_kwargs or {}
        self.process_links = process_links or _identity
        self.process_request = process_request or _identity_process_request
        self.follow = follow if follow is not None else not callback

    def _compile(self, spider):
        self.callback = _get_method(self.callback, spider)
        self.errback = _get_method(self.errback, spider)
        self.process_links = _get_method(self.process_links, spider)
        self.process_request = _get_method(self.process_request, spider)



class CrawlSpider(Spider):

    rules: Sequence[Rule] = ()

    def __init__(self, *a, **kw):
        super().__init__(*a, **kw)
        self._compile_rules()

    def _parse(self, response, **kwargs):
        return self._parse_response(
            response=response,
            callback=self.parse_start_url,
            cb_kwargs=kwargs,
            follow=True,
        )

    def parse_start_url(self, response, **kwargs):
        return []


    def process_results(self, response, results):
        return results

    def _build_request(self, rule_index, link):
        return Request(
            url=link.url,
            callback=self._callback,
            errback=self._errback,
            meta=dict(rule=rule_index, link_text=link.text),
        )

    def _requests_to_follow(self, response):
        if not isinstance(response, HtmlResponse):
            return
        seen = set()
        for rule_index, rule in enumerate(self._rules):
            links = [lnk for lnk in rule.link_extractor.extract_links(response)
                     if lnk not in seen]
            for link in rule.process_links(links):
                seen.add(link)
                request = self._build_request(rule_index, link)
                yield rule.process_request(request, response)

    def _callback(self, response):
        rule = self._rules[response.meta['rule']]
        return self._parse_response(response, rule.callback, rule.cb_kwargs, rule.follow)

    def _errback(self, failure):
        rule = self._rules[failure.request.meta['rule']]
        return self._handle_failure(failure, rule.errback)

    def _parse_response(self, response, callback, cb_kwargs, follow=True):
        if callback:
            cb_res = callback(response, **cb_kwargs) or ()
            cb_res = self.process_results(response, cb_res)
            for request_or_item in iterate_spider_output(cb_res):
                yield request_or_item

        if follow and self._follow_links:
            for request_or_item in self._requests_to_follow(response):
                yield request_or_item

    def _handle_failure(self, failure, errback):
        if errback:
            results = errback(failure) or ()
            for request_or_item in iterate_spider_output(results):
                yield request_or_item

    def _compile_rules(self):
        self._rules = []
        for rule in self.rules:
            self._rules.append(copy.copy(rule))
            self._rules[-1]._compile(self)

    @classmethod
    def from_crawler(cls, crawler, *args, **kwargs):
        spider = super().from_crawler(crawler, *args, **kwargs)
        spider._follow_links = crawler.settings.getbool('CRAWLSPIDER_FOLLOW_LINKS', True)
        return spider

```

## Scrapy分布式爬虫

- 概念:我们需要搭建一个分布式的机群，让其对一组资源进行分布联合爬取。
- 作用: 提升爬取数据的效率

### 实现分布式：

#### 安装scrapy-redis 组件

> 原生的scrapy是不能实现分布式爬虫的，必须要让scrapy结合scrapy-redis组件

原生scrapy中多台机器的五大核心组件是没有关系的:
- 调度器不可以被分布式机群共享
- 管道不可以被分布式机群共享

爬取的url:如果能够让多台机器的调度器共享,那么就可以让这多台机器共享需要爬取的资源

下载的数据:管道能被共享那么就可以实现数据的汇总

##### scrapy-redis组件的作用

- 可以给原生的scrapy框架提供可以被共享的管道和调度器
- 实现流程:
    - 创建工程:
        - `scrapy startproject fbsPro`
    - 创建一个基于CrawlSpider的爬虫:
        - `cd fbsPro`
        - `scrapy genspider fbs`
    - 编辑爬虫文件:
        - `from scrapy_redis.spiders import RedisCrawlSpider`
        - 将`start_urls`和`allow_domains`注释
        - 添加新属性:`redis_key = 'sun'`可以被共享的调度器的名称
        - 编写数据解析相关的操作
        - 将当前爬虫继承的类改为`RedisCrawlSpider`
    - 修改配置文件settings:
        - 指定使用可以被共享的管道:
            - ITEM_PIPELINES={'scrapy_redis.pipelines.RedisPipeline':400}
        - 指定调度器:
            - 指定使用scrapy-redis组件自己的调度器:
                - `SCHEDULER = "scrapy_redis.scheduler.Scheduler"`
            - 配置调度器是否要持久化:
                - `SCHEDULER_PERSIST = True` 调度器是否需要持久化，也就是当前爬虫结束了，要不要清空redis中请求队列和去重指纹的set
            - 增加去重容器类的配置，使用redis的set集合来存储请求的指纹数据
    - redis相关操作配置:
        - 配置redis的配置文件:
            - linux/mac:redis.conf
            - windows:redis.windows.conf
            - 打开配置文件修改:
                - `# bind 127.0.0.1`
                - 关闭保护模式:`protected-mode no`
            - 结合配置文件开启redis服务:
                - `redis-server` 配置文件
            - 启动客户端:
                - `redis-cli`
    - 执行工程:
        - `srapy runspider ***.py` 
    - 向调度器的队列中放入起始url:
        - 调度器队列:redis客户端中
            - lpush *** start_url
    - 爬取到的数据存储在了redis的proName:items这个数据结构中
